{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 3-1. 프로젝트: ResNet Ablation Study"
      ],
      "metadata": {
        "id": "gKz7Vr_4LzfP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKnuXjquJIyK",
        "outputId": "a1479ad4-3988-47db-a185-557421cbb336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n",
            "1.22.4\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)\n",
        "print(np.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) ResNet 기본 블록 구성하기\n",
        "\n",
        "- ResNet-34와 ResNet-50에서 사용되는 블록의 공통점\n",
        "\n",
        ": conv block이라고 불리는 블록 구조를 각각 3, 4, 6, 3개씩 반복해서 쌓은 형태\n",
        "\n",
        "- ResNet-34와 ResNet-50에서 사용되는 블록의 차이점\n",
        "\n",
        ": ResNet-34의 경우 Block은 3x3 kernel인 Convolution layer로만 구성되어있지만, ResNet-50은 1x1 Convolution이 앞뒤로 붙어 더 많은 레이어를 한 블록 내에 가지고 있음\n",
        "\n",
        "# 2) ResNet-34, ResNet-50 Complete Model\n",
        "\n",
        "[코드 작성]\n",
        "VGG와 같이 블록을 만드는 함수를 사용해서 직접 전체 모델을 만들어 봅시다. ResNet-34와 ResNet-50의 차이에 따라 달라지는 구성(configuration)을 함수에 전달해서 같은 생성 함수 build_resnet()를 통해서 ResNet의 여러 가지 버전들을 모두 만들어 낼 수 있도록 해야 합니다."
      ],
      "metadata": {
        "id": "nbS_muo2L3se"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "얕은 모델의 경우 BasicBlock을 사용함\n",
        "\n",
        "깊은 모델의 경우 BottleneckBlock을 사용함"
      ],
      "metadata": {
        "id": "b3VDCn-yO3pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import copy\n",
        "from collections import namedtuple\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "RXzUSKehSI6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageTransform() :\n",
        "    def __init__(self, resize, mean, std) :\n",
        "        self.data_transform = {\n",
        "            'train' : transforms.Compose([\n",
        "                transforms.RandomResizedCrop(resize, scale=(0.5, 1.0)),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ]),\n",
        "            'val' : transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(resize),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ])\n",
        "        }\n",
        "\n",
        "    def __call__(self, img, phase) :\n",
        "        return self.data_transform[phase](img)"
      ],
      "metadata": {
        "id": "JHb7Wny_SakC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define variables\n",
        "size = 224\n",
        "mean = (0.485, 0.456, 0.456)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "-GAdxr-bSfMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#이미지 레이블 구분\n",
        "\n",
        "class DogvsCatDataset(Dataset) :\n",
        "    def __init__(self, file_list, transform=None, phase='train') :\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "        self.phase = phase\n",
        "\n",
        "    def __len__(self) :\n",
        "        return len(self.file_list)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx) :\n",
        "        img_path = self.file_list[idx]\n",
        "        img = Image.open(img_path)\n",
        "        img_transformed = self.transform(img, self.phase)\n",
        "\n",
        "        label = img_path.split('/')[-1].split('.')[0]\n",
        "        if label == 'dog' :\n",
        "            label = 1\n",
        "\n",
        "        elif label == 'cat' :\n",
        "            label = 0\n",
        "\n",
        "        return img_transformed, label"
      ],
      "metadata": {
        "id": "U_WjSmC2Svc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#이미지 데이터셋 정의\n",
        "\n",
        "\n",
        "cat_images_filepaths = [\"/content/sample_data/dogs-vs-cats/train\"]\n",
        "dog_images_filepaths = [\"/content/sample_data/dogs-vs-cats/train\"]\n",
        "\n",
        "image_filepaths = [*cat_images_filepaths, *dog_images_filepaths]\n",
        "correct_images_filepaths = [i for i in image_filepaths if cv2.imread(i) is not None]\n",
        "train_images_filepaths = correct_images_filepaths[:400]\n",
        "val_images_filepaths = correct_images_filepaths[400:-10]\n",
        "test_images_filepaths = correct_images_filepaths[-10:]\n",
        "\n",
        "train_dataset = DogvsCatDataset(train_images_filepaths, transform=ImageTransform(size, mean, std),\n",
        "                                 phase='train')\n",
        "val_dataset = DogvsCatDataset(val_images_filepaths, transform=ImageTransform(size, mean, std),\n",
        "                               phase='val')\n",
        "index = 0\n",
        "\n",
        "\n",
        "print(train_dataset.__getitem__(index)[0].size())\n",
        "print(train_dataset.__getitem__(index)[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "_zIFwbwnS3bf",
        "outputId": "8377e6b8-0c60-4042-9cfc-c18adfe6dcda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-2d6ba04aa450>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                                phase='val')\n\u001b[1;32m     17\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-a87b6a7cfdd2>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mimg_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data to memory\n",
        "train_iterator = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_iterator = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "dataloader_dict = {'train' : train_iterator, 'val' : valid_iterator}\n",
        "\n",
        "batch_iterator = iter(train_iterator)\n",
        "inputs, label = next(batch_iterator)\n",
        "print(inputs.size())\n",
        "print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "P094Kji9TUIQ",
        "outputId": "dccb050a-b5dd-4856-c872-6c26796159e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-647683d051b0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load data to memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvalid_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataloader_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module) :\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=False) :\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                              stride=stirde, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                              stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        if downsample : # 다운샘플이 적용되는 부분(출력 데이터크기가 다를 경우 사용)\n",
        "            conv = nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                            stride=stride, bias=False)\n",
        "\n",
        "            bn = nn.BatchNorm2d(out_channels)\n",
        "            downsample = nn.Sequential(conv, bn)\n",
        "\n",
        "        else :\n",
        "            downsample = None\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x) :\n",
        "        i = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        if self.downsample is not None :\n",
        "            i = self.downsample(i)\n",
        "\n",
        "        x += i # identity mapping\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "m3AnMGr6Vcu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module) :\n",
        "    expansion = 4 # 병목 블록을 정의하기 위한 하이퍼파라미터\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=False) :\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                              stride=1, bias=False) # 1x1 합성곱층\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                              stride=stride, padding=1, bias=False)\n",
        "\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                              stride=stride, padding=1, bias=False)\n",
        "        self.conv3 = nn.Conv2d(out_channels, self.expansion * out_channels,\n",
        "                               kernel_size=1, stride=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        if downsample :\n",
        "            conv = nn.Conv2d(in_channels, self.expansion*out_channels, kernel_size=1,\n",
        "                            stride=stride, bias=False)\n",
        "            bn = nn.BatchNorm2d(self.expansion*out_channels)\n",
        "            downsample = nn.Sequential(conv, bn)\n",
        "        else :\n",
        "            downsample = None\n",
        "\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x) :\n",
        "        i = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        if self.downsample is not None :\n",
        "            i = self.downsample(i)\n",
        "\n",
        "        x += i\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "WZ9pKVX1Vf2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module) :\n",
        "   def __init__(self, config, output_dim, zero_init_residual=False) :\n",
        "       super().__init__()\n",
        "\n",
        "       block, n_blocks, channels = config\n",
        "       self.in_channels = channels[0]\n",
        "       assert len(n_blocks) == len(channels) == 4\n",
        "\n",
        "       self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2,\n",
        "                             padding=3, bias=False)\n",
        "       self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "       self.relu = nn.ReLU(inplace=True)\n",
        "       self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "       self.layer1 = self.get_resnet_layer(block, n_blocks[0], channels[0])\n",
        "       self.layer2 = self.get_resnet_layer(block, n_blocks[1], channels[1], stride=2)\n",
        "       self.layer3 = self.get_resnet_layer(block, n_blocks[1], channels[2], stride=2)\n",
        "       self.layer4 = self.get_resnet_layer(block, n_blocks[1], channels[3], stride=2)\n",
        "\n",
        "       self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "       self.fc = nn.Linear(self.in_channels, output_dim)\n",
        "\n",
        "\n",
        "       if zero_init_residual :\n",
        "           for m in self.modules() :\n",
        "               if isinstance(m, BottleNeck) :\n",
        "                   nn.init.constant_(m.bn3.weight, 0)\n",
        "               elif isinstance(m, BasicBlock) :\n",
        "                   nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "   def get_resnet_layer(self, block, n_blocks, channels, stride=1) :\n",
        "       layers = []\n",
        "       if self.in_channels != block.expansion * channels :\n",
        "           downsample = True\n",
        "       else :\n",
        "           downsample = False\n",
        "\n",
        "       layers.append(block(self.in_channels, channels, stride, downsample))\n",
        "\n",
        "       for i in range(1, n_blocks) :\n",
        "           layers.append(block(block.expansion*channels, channels))\n",
        "\n",
        "       self.in_channels = block.expansion * channels\n",
        "       return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "   def forward(self, x) :\n",
        "       x = self.conv1(x)\n",
        "       x = self.bn1(x)\n",
        "       x = self.relu(x)\n",
        "       x = self.maxpool(x)\n",
        "       x = self.layer1(x)\n",
        "       x = self.layer2(x)\n",
        "       x = self.layer3(x)\n",
        "       x = self.layer4(x)\n",
        "       x = self.avgpool(x)\n",
        "       h = x.view(x.shape[0], -1)\n",
        "       x = self.fc(h)\n",
        "       return x, h\n",
        "\n",
        "```python\n",
        "ResNetConfig = namedtuple('ResNetConfig', ['block', 'n_blocks', 'channels'])"
      ],
      "metadata": {
        "id": "QkpSWVXGVjLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#restnet config def\n",
        "\n",
        "resnet34_config = ResNetConfig(block=BasicBlock,\n",
        "                             n_blocks=[3, 4, 6, 3,],\n",
        "                             channels=[64, 128, 256, 512])\n",
        "\n",
        "resnet50_config = ResNetConfig(block=Bottleneck,\n",
        "                             n_blocks=[3, 4, 6, 3],\n",
        "                             channels=[64,128,256,512])"
      ],
      "metadata": {
        "id": "QJ-hBQZoVquQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dim = 2\n",
        "model = ResNet(resnet50_config, output_dim)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "E-1DMi0OVz_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#resnet 50\n",
        "\n",
        "# 손실함수 정의\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-7)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "vGN5VMiNV1xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_topk_accuracy(y_pred, y, k=2) :\n",
        "    with torch.no_grad() :\n",
        "        batch_size = y.shape[0]\n",
        "        _, top_pred = y_pred.topk(k, 1)\n",
        "        top_pred = top_pred.t()\n",
        "        correct = top_pred.eq(y.view(1, -1).expand_as(top_pred))\n",
        "        correct_1 = correct[:1].reshape(-1).float().sum(0, keepdim=True)\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        acc_1 = correct_1 / batch_size\n",
        "        acc_k = correct_k / batch_size\n",
        "\n",
        "    return acc_1, acc_k"
      ],
      "metadata": {
        "id": "3SYg6q7cV6jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, device) :\n",
        "    epoch_loss = 0\n",
        "    epoch_acc_1 = 0\n",
        "    epoch_acc_5 = 0\n",
        "    model.train()\n",
        "    for (x, y) in iterator :\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x)\n",
        "        loss = criterion(y_pred[0], y)\n",
        "\n",
        "        acc_1, acc_5 = calculate_topk_accuracy(y_pred[0], y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc_1 += acc_1.item()\n",
        "        epoch_acc_5 += acc_5.item()\n",
        "\n",
        "    epoch_loss /= len(iterator)\n",
        "    epoch_acc_1 /= len(iterator)\n",
        "    epoch_acc_5 /= len(iterator)\n",
        "\n",
        "    return epoch_loss, epoch_acc_1, epoch_acc_5"
      ],
      "metadata": {
        "id": "2Ki7CPy5V8p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion, device) :\n",
        "    epoch_loss = 0\n",
        "    epoch_acc_1 = 0\n",
        "    epoch_acc_5 = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad() :\n",
        "        for (x, y) in iterator :\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred[0], y)\n",
        "\n",
        "            acc_1, acc_5 = calculate_topk_accuracy(y_pred[0], y)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc_1 += acc_1.item()\n",
        "            epoch_acc_5 += acc_5.item()\n",
        "\n",
        "        epoch_loss /=len(iterator)\n",
        "        epoch_acc_1 /= len(iterator)\n",
        "        epoch_acc_5 /= len(iterator)\n",
        "\n",
        "        return epoch_loss, epoch_acc_1, epoch_acc_5"
      ],
      "metadata": {
        "id": "VGzZHK_oV-ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time) :\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "a34Hxp_TWAUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model 학습\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs) :\n",
        "    start_time = time.monotonic()\n",
        "\n",
        "    train_loss, train_acc_1, train_acc_5 = train(model, train_iterator, optimizer,\n",
        "                                                criterion, device)\n",
        "    valid_loss, valid_acc_1, valid_acc_5 = evaluate(model, valid_iterator, criterion,\n",
        "                                                   device)\n",
        "\n",
        "    if valid_loss < best_valid_loss :\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '../080289-main/chap06/data/ResNet-model.pt')\n",
        "\n",
        "    end_time = time.monotonic()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch : {epoch+1:02} | Epoch Time : {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss : {train_loss:.3f} | Train Acc @1 : {train_acc_1*100:6.2f}% | Train Acc @5 : {train_acc_5*100:6.2f}%')\n",
        "    print(f'\\tValid Loss : {valid_loss:.3f} | Valid Acc @1 : {valid_acc_1*100:6.2f}% | Valid Acc @5 : {valid_acc_5*100:6.2f}%')"
      ],
      "metadata": {
        "id": "qrCG_uYYWCFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트 데이터셋으로 모델 예측\n",
        "\n",
        "import pandas as pd\n",
        "id_list = []\n",
        "pred_list = []\n",
        "_id = 0\n",
        "\n",
        "with torch.no_grad() :\n",
        "    for test_path in test_images_filepaths :\n",
        "        img = Image.open(test_path)\n",
        "        _id = test_path.split('/')[-1].split('.')[1]\n",
        "        transform = ImageTransform(size, mean, std)\n",
        "        img = transform(img, phase='val')\n",
        "        img = img.unsqueeze(0)\n",
        "        img = img.to(device)\n",
        "\n",
        "        model.eval()\n",
        "        outputs = model(img)\n",
        "        preds = F.softmax(outputs[0], dim=1)[:, 1].tolist()\n",
        "        id_list.append(_id)\n",
        "        pred_list.append(preds[0])\n",
        "\n",
        "res = pd.DataFrame({\n",
        "        'id' : id_list,\n",
        "        'label' : pred_list\n",
        "})\n",
        "\n",
        "res.sort_values(by='id', inplace=True)\n",
        "res.reset_index(drop=True, inplace=True)\n",
        "\n",
        "res.to_csv('../080289-main/chap06/data/ResNet.csv', index=False)\n",
        "res.head(10)"
      ],
      "metadata": {
        "id": "Yt3Otx30WHSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#예측결과 시각화 하여 확인\n",
        "import pandas as pd\n",
        "id_list = []\n",
        "pred_list = []\n",
        "_id = 0\n",
        "\n",
        "with torch.no_grad() :\n",
        "    for test_path in test_images_filepaths :\n",
        "        img = Image.open(test_path)\n",
        "        _id = test_path.split('/')[-1].split('.')[1]\n",
        "        transform = ImageTransform(size, mean, std)\n",
        "        img = transform(img, phase='val')\n",
        "        img = img.unsqueeze(0)\n",
        "        img = img.to(device)\n",
        "\n",
        "        model.eval()\n",
        "        outputs = model(img)\n",
        "        preds = F.softmax(outputs[0], dim=1)[:, 1].tolist()\n",
        "        id_list.append(_id)\n",
        "        pred_list.append(preds[0])\n",
        "\n",
        "res = pd.DataFrame({\n",
        "        'id' : id_list,\n",
        "        'label' : pred_list\n",
        "})\n",
        "\n",
        "res.sort_values(by='id', inplace=True)\n",
        "res.reset_index(drop=True, inplace=True)\n",
        "\n",
        "res.to_csv('../080289-main/chap06/data/ResNet.csv', index=False)\n",
        "res.head(10)\n",
        "class_ = classes = {0 : 'cat', 1 : 'dog'}\n",
        "\n",
        "def display_image_grid(images_filepaths, predicted_labels=(), cols=5) :\n",
        "    rows = len(images_filepaths) // cols\n",
        "    fig , ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n",
        "    for i, image_filepath in enumerate(images_filepaths) :\n",
        "        image = cv2.imread(image_filepath)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        a = random.choice(res['id'].values)\n",
        "        label = res.loc[res['id'] == a, 'label'].values[0]\n",
        "\n",
        "        if label > 0.5 :\n",
        "            label = 1\n",
        "        else :\n",
        "            label = 0\n",
        "\n",
        "        ax.ravel()[i].imshow(image)\n",
        "        ax.ravel()[i].set_title(class_[label])\n",
        "        ax.ravel()[i].set_axis_off()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "display_image_grid(test_images_fileapths)"
      ],
      "metadata": {
        "id": "1DW13bNvWTHR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}